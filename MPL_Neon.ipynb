{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron for MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A perceptron is a linear classifer to separate data between two features\n",
    "- Two possible output results:\n",
    "\n",
    "    1) Positive = 1\n",
    "    \n",
    "    2) Negative = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_binary.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_binary.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feedfoward artificial neural network with one or more hidden layers\n",
    "- Able to distinguish data that is not lineraly separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.safaribooksonline.com/library/view/getting-started-with/9781786468574/graphics/B05474_04_05.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://www.safaribooksonline.com/library/view/getting-started-with/9781786468574/graphics/B05474_04_05.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Called \"hidden\" layers because their values are not given in the input data\n",
    "- The previous layer of the model provides a new representation of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.stack.imgur.com/x9FAm.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://i.stack.imgur.com/x9FAm.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neon.util.argparser import NeonArgparser\n",
    "from neon.backends import gen_backend\n",
    "from neon.callbacks.callbacks import ProgressBarCallback\n",
    "from neon.data import MNIST\n",
    "from neon.data import ArrayIterator\n",
    "from neon.data import MNIST\n",
    "from neon.initializers import Gaussian\n",
    "from neon.layers import Affine\n",
    "from neon.transforms import Rectlin, Softmax\n",
    "from neon.models import Model\n",
    "from neon.layers import GeneralizedCost\n",
    "from neon.transforms import CrossEntropyMulti\n",
    "from neon.optimizers import GradientDescentMomentum\n",
    "from neon.callbacks.callbacks import Callbacks\n",
    "from neon.transforms import Misclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This method just downloads the pre-processed MNIST Dataset using Neon\n",
    "    Built-in functions.\n",
    "    \"\"\"\n",
    "    mnist = MNIST()\n",
    "    (X_train, y_train), (X_test, y_test), nclass = mnist.load_data()\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Test_train(X_train,y_train,X_test,y_test,nclass):\n",
    "    \"\"\"\n",
    "    This loads the Train and Test dataset into memory and returns an \n",
    "    Iterator for the model to use\n",
    "    \"\"\"\n",
    "#     (num_examples, num_features) = (60000, 784).\n",
    "    train_set = ArrayIterator(X_train, y_train, nclass=nclass)\n",
    "#     (num_examples, num_features) = (10000, 784).\n",
    "    test_set = ArrayIterator(X_test, y_test, nclass=nclass)\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights():\n",
    "    \"\"\"\n",
    "    This is a function to Initialize the weights using a Normal Gaussian Distribution \n",
    "    \"\"\"\n",
    "    init_norm = Gaussian(loc=0.0, scale=0.01)\n",
    "    return init_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affine (i.e. fully-connected) layer \n",
    "# Normal Fully Connected Layer\n",
    "def createAffine(n_hidden,activation_func,weight_init):\n",
    "    \"\"\"\n",
    "    This method creates a AFFINE (Fully Connected Hidden Layer). \n",
    "    This is a normal linear layer used in Neural Networks.\n",
    "    \n",
    "    Params:\n",
    "    @ n_hidden        : No of hidden units to use in the layer\n",
    "    @ activation_func : The activation function to be used in the layer\n",
    "    @ weight_init     : The weight Initializer to be used to initialize weights in this layer\n",
    "    \n",
    "    Returns:\n",
    "    Affine Layer Object\n",
    "    \"\"\"\n",
    "    return Affine(nout=n_hidden, init=weight_init, activation=activation_func)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "def create_hidden_layers(layers,\n",
    "                         no_of_layers,\n",
    "                         hidden_unit_list,\n",
    "                         activ_func_list,\n",
    "                         weight_init_list):\n",
    "    \"\"\"\n",
    "    This method creates the complete hidden layer architecture.\n",
    "    \n",
    "    Params:\n",
    "    @ layers           : This is a list containing any previous layers or no layers \n",
    "    @ no_of_layers     : The no of hidden layers to create\n",
    "    @ hidden_unit_list : A list of no of hidden units to use in each layer\n",
    "    @ activ_func_list  : A list of Activation functions to be used in each layer\n",
    "    @ weight_init_list : A list of weight initializers to be used in each layer\n",
    "    \n",
    "    Returns:\n",
    "    A list of the newly created hidden layers\n",
    "    \"\"\"\n",
    "    for index,num in enumerate(hidden_unit_list):\n",
    "        layers.append(createAffine(n_hidden=num,\n",
    "                                   weight_init=weight_init_list[index],\n",
    "                                   activation_func=activ_func_list[index]))\n",
    "    return layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(layers):\n",
    "    \"\"\"\n",
    "    This is a function that just initializes the Neural Network model by using the predefined layers.\n",
    "    \n",
    "    Params:\n",
    "    @ layers : A list containing the different layer objects representing the Network\n",
    "    \n",
    "    Returns:\n",
    "    An NN model Object\n",
    "    \"\"\"\n",
    "    mlp = Model(layers=layers)\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cost(cost_func):\n",
    "    \"\"\"\n",
    "    This is an initialization function to define the type of LOSS/COST function to use.\n",
    "    \n",
    "    Params:\n",
    "    @ cost_func : the Type of Cost func to use\n",
    "    \n",
    "    Returns:\n",
    "    A cost Function Object for the Network\n",
    "    \"\"\"\n",
    "    cost = GeneralizedCost(costfunc=cost_func)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_optimizer(learning_rate=0.1,momentum_coefficient=0.9):\n",
    "    \"\"\"\n",
    "    Initilizes an Optimizer object \n",
    "    \n",
    "    Params:\n",
    "    @ learning_rate : The learning rate\n",
    "    \n",
    "    @ Momentum_cofficient: Regularization Parameter\n",
    "    \"\"\"\n",
    "    optimizer = GradientDescentMomentum(learning_rate, momentum_coef=momentum_coefficient)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neon provides an API for calling operations during the model fit (see Callbacks). \n",
    "# Here we set up the default callback, which is displaying a progress bar for each epoch.\n",
    "def init_callback(model,evaluation_set):\n",
    "    \"\"\"\n",
    "    Initializes Callback Function to check the Progress of the model during training\n",
    "    \n",
    "    Params:\n",
    "    @ model : The NN model object\n",
    "    @ evaluation_set: The Data to check against\n",
    "    \"\"\"\n",
    "    callbacks = Callbacks(model, eval_set=evaluation_set)\n",
    "#     callbacks.add_progress_bar_callback()\n",
    "    callbacks.add_callback(ProgressBarCallback())\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model,train_set,optimizer,epochs,cost,callbacks):\n",
    "    \"\"\"\n",
    "    This function runs the model\n",
    "    \n",
    "    Params:\n",
    "    @ model     : The model Object\n",
    "    @ train_set : The training data set that contains both X,Y\n",
    "    @ Optimizer : The optimizer to use for GD\n",
    "    @ epochs    : The no of epochs to run for\n",
    "    @ cost      : The cost Function to use\n",
    "    @ callbacks : The callback function to check progress\n",
    "    \"\"\"\n",
    "    model.fit(train_set, \n",
    "              optimizer=optimizer, \n",
    "              num_epochs= epochs,\n",
    "              cost=cost,\n",
    "              callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model,test_set):\n",
    "    \"\"\"\n",
    "    This function retrieves the results\n",
    "    \"\"\"\n",
    "    results = model.get_outputs(test_set)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(model,test_set,metric):\n",
    "    \"\"\"\n",
    "    This function checks the error rate of the model\n",
    "    \"\"\"\n",
    "    error = model.eval(test_set, metric=metric)*100\n",
    "    print('Misclassification error = %.1f%%' % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    gen_backend(backend='cpu', batch_size=128)\n",
    "#     Step1: Load MNIST Data \n",
    "    X_train, y_train, X_test, y_test = load_data()\n",
    "#     Step2: Create the Data Loader for test and train\n",
    "    train_set, test_set = get_Test_train(X_train,y_train,X_test,y_test,nclass=10)\n",
    "#     Step3: Create Layers for the model\n",
    "    layers = []\n",
    "    no_of_layers = 3\n",
    "    hidden_unit_list = [30,50,10]\n",
    "    activ_func_list = [Rectlin(),Rectlin(),Softmax()]\n",
    "    weight_init_list = [initialize_weights() for i in range(len(hidden_unit_list))]\n",
    "    layers = create_hidden_layers(layers,\n",
    "                                  no_of_layers,\n",
    "                                  hidden_unit_list,\n",
    "                                  activ_func_list,\n",
    "                                  weight_init_list)\n",
    "    mlp = init_model(layers=layers)\n",
    "    cost = init_cost(cost_func=CrossEntropyMulti())\n",
    "   \n",
    "    learning_rate = 0.1\n",
    "    momentum_coef = 0.9\n",
    "    \n",
    "    optimizer = init_optimizer(learning_rate,momentum_coef)\n",
    "    \n",
    "    callback = init_callback(model=mlp,evaluation_set=test_set)\n",
    "    \n",
    "    run_model(model=mlp,\n",
    "              train_set=train_set,\n",
    "              optimizer=optimizer,\n",
    "              epochs=10,\n",
    "              cost=cost,\n",
    "              callbacks=callback)\n",
    "    \n",
    "    results = get_results(model = mlp,test_set=test_set) \n",
    "    \n",
    "    error_rate(model=mlp,test_set=test_set,metric=Misclassification())\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-2b29415775d6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     Step2: Create the Data Loader for test and train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_Test_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnclass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#     Step3: Create Layers for the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-ddcc5e5e0b65>\u001b[0m in \u001b[0;36mget_Test_train\u001b[0;34m(X_train, y_train, X_test, y_test, nclass)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     (num_examples, num_features) = (60000, 784).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArrayIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnclass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#     (num_examples, num_features) = (10000, 784).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArrayIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnclass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/neon-2.3.0-py3.5.egg/neon/data/dataiterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, X, y, nclass, lshape, make_onehot, name)\u001b[0m\n\u001b[1;32m    150\u001b[0m                     lambda _in, _out: self.be.onehot(_in, axis=0, out=_out))\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtranspose_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# Shallow copies for appending, iterating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/neon-2.3.0-py3.5.egg/neon/data/dataiterator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    150\u001b[0m                     lambda _in, _out: self.be.onehot(_in, axis=0, out=_out))\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtranspose_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# Shallow copies for appending, iterating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/neon-2.3.0-py3.5.egg/neon/data/dataiterator.py\u001b[0m in \u001b[0;36mtranspose_gen\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# Helpers to make dataset, minibatch, unpacking function for transpose and onehot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mtranspose_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             return (self.be.array(z), self.be.iobuf(z.shape[1]),\n\u001b[0m\u001b[1;32m    146\u001b[0m                     lambda _in, _out: self.be.copy_transpose(_in, _out))\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/neon-2.3.0-py3.5.egg/neon/backends/nervanacpu.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(self, ary, dtype, name, persist_values, parallel, distributed)\u001b[0m\n\u001b[1;32m    781\u001b[0m         return self.tensor_cls(\n\u001b[1;32m    782\u001b[0m             \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m             \u001b[0mary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neon.data import MNIST\n",
    "# gen_backend(backend='cpu', batch_size=128)\n",
    "# mnist = MNIST()\n",
    "# train_set = mnist.train_iter\n",
    "# test_set = mnist.valid_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
