{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Loading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "+ There are two components to working with data in neon   \n",
    "+ The first is a data iterator **(NervanaDataIterator)**, that feeds the model with minibatches of data during training or evaluation  \n",
    "+ The second is a dataset **(Dataset)** class, which handles the loading and preprocessing of the data    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Data iterators are python iterables in that they implement the ```__iter__``` method, which returns a new minibatch of data with each call  \n",
    "+ There are 2 kinds of Iterator classes that are used in Neon, depending on the size of the data.\n",
    "+ Custom Iterators can also be built for certain use cases by extending the below mentioned classes.\n",
    "+ If your data is small enough to fit into memory, you use the ```ArrayIterator``` Class in Neon  \n",
    "+ If your data is too large, you can save your data in the HDF5 format and use the ```HDF5Iterator``` Class, to load chunks of data to send to the model. This approach is flexible for any type of data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArrayIterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The ArrayIterator class provides for iteration over minibatches of data that has been preloaded into memory as numpy arrays. \n",
    "+ This iterator supports classification, regression, and autoencoder tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Below is an example of a classification problem statement with images where we load in 10,000 images. \n",
    "+ Each image is 32x32 pixels with 3 color channels (R, G, B), for a total of 32×32×3=3,072 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neon.data import ArrayIterator\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "X are the features and y are the labels.\n",
    "The data in X must have shape (# examples, feature size)\n",
    "\"\"\"\n",
    "X = np.random.rand(10000,3072) # X.shape = (10000, 3072)\n",
    "\"\"\"\n",
    "\n",
    "For classification, the labels y must have shape (# examples, 1). y must also\n",
    "consist of integers from 0 to nclass-1, where nclass is the number of categories.\n",
    "\"\"\"\n",
    "y = np.random.randint(0,10,10000) # y.shape = (10000, )\n",
    "\n",
    "\"\"\"\n",
    "The features X and labels y are passed to ArrayIterator be loaded into the backend\n",
    "nclass, the number of classes, is set to 10\n",
    "lshape, the local shape of the features, is set to (3,32,32) to represent\n",
    "        the the image dimensions: 32x32 pixels with 3 channels\n",
    "\"\"\"\n",
    "train = ArrayIterator(X=X, y=y, nclass=10, lshape=(3,32,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Importantly, the labels y for classification should be integers from **0** to **K−1**, where **K** is the number of classes. \n",
    "+ These labels are stored in the backend in a one-hot representation. \n",
    "+ This means that if we have **N** labels with **K** classes, the labels will be stored in a **N×K** binary matrix. \n",
    "+ Each column will be all zeros except at the **k-th** element, which will be **one**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{split}y = (0, 2, 0, 1, 2, 3) \\rightarrow \\left( \\begin{array}{cccccc}\n",
    "1 & 0  & 0 & 0\\\\\n",
    "0 & 0  & 1 & 0\\\\\n",
    "1 & 0  & 0 & 0\\\\\n",
    "0 & 1  & 0 & 0\\\\\n",
    "0 & 0  & 1 & 0\\\\\n",
    "0 & 0  & 0 & 1 \\end{array}  \\right).\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ In regression, the model output for each training example is a vector ***_y^_*** that is compared against a desired vector ***_y_*** with a cost function (such as mean squared error).\n",
    "+ We first create the iterator. By default, ArrayIterator assumes classification, so for regression we must set ```make_onehot = False``` to turn off the one-hot representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neon.data import ArrayIterator\n",
    "import numpy as np\n",
    "\n",
    "# We generate some random data as X\n",
    "X = np.random.rand(1000, 1)\n",
    "# Add a bit of Noise to our data\n",
    "y = 2*X + 1 + 0.01*np.random.randn(1000, 1)\n",
    "\n",
    "train = ArrayIterator(X=X, y=y, make_onehot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Autoencoders are a special case of regression where the desired outputs ***_y_*** are the input features ***X***. \n",
    "+ For convenience, you can exclude passing the labels ***_y_*** to the iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example construction of ArrayIterator for Autoencoder task with MNIST\n",
    "from neon.data import MNIST\n",
    "from neon.data import ArrayIterator\n",
    "\n",
    "mnist = MNIST()\n",
    "\n",
    "# load the MNIST data\n",
    "(X_train, y_train), (X_test, y_test), nclass = mnist.load_data()\n",
    "\n",
    "# Set input and target to X_train\n",
    "train = ArrayIterator(X_train, lshape=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sequence data, where data are fed to the model across multiple time steps, the shape of the input data can depend on your usage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Word Embeddings***\n",
    "+ Often, data such as sentences are encoded as a vector sequence of integers, where each integer corresponds to a word in  the vocabulary.  \n",
    "+ This encoding is often used in conjunction with embedding layers.  \n",
    "+ In this case, the input data should be formatted to have shape ***_(T,N)_***, where **_T_** is the number of time steps and **N** is the batch size.  \n",
    "+ The embedding layer takes this input and provides as output to a subsequent recurrent neural network data of shape **(F,T∗N)**, where **F** is the number of features (in this case, the embedding dimension). For an example, see imdb_lstm.py  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Time Series***  \n",
    "+ Time series data should be formatted to have shape ***(F,T∗N)***  \n",
    "  + where   \n",
    "  **F** is the number of features  \n",
    "  **T** is the number of timesteps/interval  \n",
    "  **N** is the Batch Size (user controlled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is HDF5 ?**  \n",
    "\n",
    "+ HDF5 is a unique technology suite that makes possible the management of extremely large and complex data collections.\n",
    "+ A versatile data model that can represent very complex data objects and a wide variety of metadata.\n",
    "+ A completely portable file format with no limit on the number or size of data objects in the collection.\n",
    "+ A software library that runs on a range of computational platforms, from laptops to massively parallel systems, and implements a high-level API with C, C++, Fortran 90, and Java interfaces.\n",
    "+ A rich set of integrated performance features that allow for access time and storage space optimizations.\n",
    "+ Tools and applications for managing, manipulating, viewing, and analyzing the data in the collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```HDF5Iterator``` uses an HDF5 formatted data file to store the input and target data arrays so the data size is not limited by on-host and/or on-device memory capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the ```HDF5Iterator```, the data arrays need to be stored in an HDF5 file with the following format:  \n",
    "+ The input data is in an HDF5 dataset named input and the target output, if needed, in a dataset named output. The data arrays are of the same format as the arrays used to initialize the ```ArrayIterator``` class.  \n",
    "+ The input data class also requires an attribute named lshape which specifies the shape of the flattened input data array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For alternate target label formats, such as converting the targets to a one-hot vector, or for autoencoder data, the ```HDF5IteratorOneHot``` and ```HDF5IteratorAutoencoder``` subclasses are included. These subclasses demonstrate how to extend the ```HDF5Iterator``` to handle different input and target data formats or transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neon.data import HDF5IteratorOneHot, MNIST\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# load up the mnist data set\n",
    "dataset = MNIST(path=args.data_dir)\n",
    "# split into train and tests sets\n",
    "(X_train, y_train), (X_test, y_test), nclass = dataset.load_data()\n",
    "\n",
    "datsets = {'train': (X_train, y_train),\n",
    "           'test': (X_test, y_test)}\n",
    "\n",
    "for ky in ['train', 'test']:\n",
    "    df = h5py.File('mnist_%s.h5' % ky, 'w')\n",
    "\n",
    "    # input images\n",
    "    in_dat = datsets[ky][0]\n",
    "    df.create_dataset('input', data=in_dat)\n",
    "    df['input'].attrs['lshape'] = (1, 28, 28)  # (C, H, W)\n",
    "\n",
    "    target = datsets[ky][1].reshape((-1, 1))  # make it a 2D array\n",
    "    df.create_dataset('output', data=target)\n",
    "    df['output'].attrs['nclass'] = 10\n",
    "    df.close()\n",
    "\n",
    "# setup a training set iterator\n",
    "# use the iterator that generates 1-hot output. other HDF5Iterator (sub) classes are\n",
    "# available for different data layouts\n",
    "train_set = HDF5IteratorOneHot('mnist_train.h5')\n",
    "valid_set = HDF5IteratorOneHot('mnist_test.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Dataset class is a base class for commonly-used datasets.  \n",
    "+ We recommend creating an object class for your dataset that handles the loading and preprocessing of the data. \n",
    "+ Datasets should implement ```gen_iterators()```, which returns a dictionary data iterator used for training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Neon provides dataset objects for handling many stock datasets.***\n",
    "  \n",
    "  \n",
    "They are as follows:  \n",
    "+ MNIST, is a dataset of handwritten digits, consisting of 60,000 training samples and 10,000 test samples. Each image is 28x28 greyscale pixels.  \n",
    "+ CIFAR10, is a dataset consisting of 50,000 training samples and 10,000 test samples. There are 10 categories and each sample is a 32x32 RGB color image.  \n",
    "+ Text datasets (e.g. Penn Treebank, Hutter Prize, and Shakespeare), we have object classes for loading, and sometimes pre-processing, the data.  \n",
    "+ ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neon.data import MNIST\n",
    "\n",
    "mnist = MNIST(path='path/to/save/downloadeddata/')\n",
    "train_set = mnist.train_iter\n",
    "valid_set = mnist.valid_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neon.data import CIFAR10\n",
    "\n",
    "cifar10 = CIFAR10()\n",
    "train = cifar10.train_iter\n",
    "test = cifar10.valid_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Penn Tree Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neon.data import PTB\n",
    "\n",
    "# download Penn Treebank and parse at the word level\n",
    "ptb = PTB(time_steps, tokenizer=\"newline_tokenizer\")\n",
    "train_set = ptb.train_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low level dataset operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Some applications require access to the underlying data to generate more complex data iterators. \n",
    "+ This can be done by using the load_data method of the DataSet class and its subclasses. \n",
    "+ The method returns the data arrays which are used to generate the data iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, the code below shows how to generate a data iterator to train an autoencoder on the MNIST dataset\n",
    "\n",
    "from neon.data import MNIST\n",
    "from neon.data import ArrayIterator\n",
    "\n",
    "mnist = MNIST()\n",
    "# get the raw data arrays, both train set and validation set\n",
    "(X_train, y_train), (X_test, y_test), nclass = mnist.load_data()\n",
    "\n",
    "# generate and ArrayIterator with no target data\n",
    "# this will return the image itself as the target\n",
    "train = ArrayIterator(X_train, lshape=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Resources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Neon Documentation : http://neon.nervanasys.com/docs/latest/index.html  \n",
    "+ Neon Examples : https://github.com/NervanaSystems/neon/tree/master/examples\n",
    "+ Github Repo : https://github.com/NervanaSystems/neon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
